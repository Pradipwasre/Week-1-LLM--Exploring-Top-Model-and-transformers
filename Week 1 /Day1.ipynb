{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions and Concepts in LLMs\n",
    "\n",
    "## 1. Tokenization\n",
    "**Definition:** The process of breaking down text into smaller units called tokens, such as words or subwords, which the model can understand and process.\n",
    "\n",
    "**Detail:** Tokenization is a crucial step in natural language processing (NLP). It allows the model to handle text data in a structured way. Tokens can be words, subwords, or even characters. Different tokenization techniques, such as Byte Pair Encoding (BPE) or WordPiece, help manage the vocabulary size and handle out-of-vocabulary words.\n",
    "\n",
    "## 2. Embeddings\n",
    "**Definition:** A numerical representation of words or phrases in a continuous vector space, capturing their meanings and relationships.\n",
    "\n",
    "**Detail:** Embeddings map words or phrases to dense vectors of real numbers, typically in a high-dimensional space. These vectors capture semantic meanings, such as word similarity and context. Popular embedding techniques include Word2Vec, GloVe, and contextual embeddings from models like BERT.\n",
    "\n",
    "## 3. Attention Mechanisms\n",
    "**Definition:** Mechanisms that allow the model to focus on different parts of the input text, assigning different levels of importance to each part.\n",
    "\n",
    "**Detail:** Attention mechanisms help the model weigh the relevance of different words or phrases when processing a sentence. Self-attention, or intra-attention, is a key component of Transformer models. It allows each word to attend to every other word in the sequence, capturing dependencies regardless of their distance.\n",
    "\n",
    "## 4. Transformer Architecture\n",
    "**Definition:** The backbone of most LLMs, consisting of multiple layers of attention mechanisms and feed-forward neural networks.\n",
    "\n",
    "**Detail:** Transformers use self-attention and feed-forward neural networks to process input sequences in parallel. This architecture enables efficient handling of long-range dependencies and parallelization, making it suitable for training large models on massive datasets. The original Transformer model has encoder and decoder stacks, though many LLMs use just the encoder or decoder.\n",
    "\n",
    "## 5. Pre-training\n",
    "**Definition:** The process of training a model on a large corpus of text to learn general language patterns and representations.\n",
    "\n",
    "**Detail:** During pre-training, models are typically trained using unsupervised or self-supervised learning objectives, such as predicting the next word in a sentence (language modeling) or filling in masked words (masked language modeling). This phase helps the model learn useful representations that can be fine-tuned for specific tasks.\n",
    "\n",
    "## 6. Fine-tuning\n",
    "**Definition:** The process of further training a pre-trained model on a smaller, task-specific dataset to adapt it for a particular application.\n",
    "\n",
    "**Detail:** Fine-tuning adjusts the model's weights to optimize performance on a specific task, such as sentiment analysis, named entity recognition, or question answering. This step leverages the general knowledge learned during pre-training and tailors it to the target task.\n",
    "\n",
    "## 7. Inference\n",
    "**Definition:** The process of using a trained model to generate or analyze text based on new input data.\n",
    "\n",
    "**Detail:** During inference, the model takes input text, processes it using its learned weights and architectures, and produces predictions or generated text. Inference can be performed on various tasks, such as text completion, translation, or summarization.\n",
    "\n",
    "# Example Code with Detailed Comments\n",
    "\n",
    "Now, let's revisit the code with detailed comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import openai\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Setting the OpenAI API key for authentication\n",
    "# Ensure you replace 'YOUR_API_KEY' with your actual API key\n",
    "openai.api_key = 'YOUR_API_KEY'\n",
    "\n",
    "# Defining headers to mimic a web browser for web requests\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "# Defining the Website class to represent and process a webpage\n",
    "class Website:\n",
    "    def __init__(self, url):\n",
    "        # Initializing with the URL of the webpage\n",
    "        self.url = url\n",
    "        \n",
    "        # Sending an HTTP GET request to fetch the webpage content\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # Parsing the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extracting the title of the webpage or setting a default value\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        \n",
    "        # Removing irrelevant tags (script, style, img, input) from the body\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        \n",
    "        # Extracting the text content of the webpage's body\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "# Defining the system prompt for the assistant\n",
    "system_prompt = \"You are an assistant that analyzes the contents of a website and provides a short summary, ignoring text that might be navigation related. Respond in markdown.\"\n",
    "\n",
    "# Defining a function to create a user prompt based on the website content\n",
    "def user_prompt_for(website):\n",
    "    # Constructing the user prompt with the website's title and text\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; please provide a short summary of this website in markdown. If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt\n",
    "\n",
    "# Defining a function to combine the system and user prompts for the API call\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]\n",
    "\n",
    "# Defining a function to summarize the content of a webpage using OpenAI\n",
    "def summarize(url):\n",
    "    # Creating an instance of the Website class\n",
    "    website = Website(url)\n",
    "    \n",
    "    # Making an API call to OpenAI to generate a summary\n",
    "    response = openai.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = messages_for(website)\n",
    "    )\n",
    "    \n",
    "    # Returning the content of the generated summary\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Defining a function to display the summary in Markdown format\n",
    "def display_summary(url):\n",
    "    # Generating the summary for the specified URL\n",
    "    summary = summarize(url)\n",
    "    \n",
    "    # Displaying the summary as Markdown\n",
    "    display(Markdown(summary))\n",
    "\n",
    "# Running the function to display the summary for a specific website\n",
    "display_summary(\"https://edwarddonner.com\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
